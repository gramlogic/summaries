# The Nature of Environments

**task environments**:
* "problems" to which rational agents are "solutions"
* Show how to specify a task environment
* Show variety of types of environments

## 2.3.1 Specifying the task environment
task environment (PEAS):
* performance measure
* environment
* actuators
* sensors

### Taxi example:
Performance measure:
* correct destination
* minimizing fuel consumption
* minimizing wear on car
* minimizing trip time / cost
* minimizing traffic violations
* minimizing disturbance to other drivers
* maximizing safety and passenger comfort
* maximizing profit

#### Remarks
Just recording this performance measure because it tells you something about the designer. The passenger is not the designer and has divergent definition of desirability.

Also worth noting: the lists of environmental factors, sensors and actuators are all completely ad hoc.

### real vs artificial environments
This doesn't matter that much. Discussion of "softbots" with complicated tasks.

## 2.3.2 Properties of task environments 
Claims:
* vast array of task environments can be categorized with a small number of dimensions
* categorization corresponds with appropriate agent design
* also applicability of principal families of techniques for agent implementation.

### Dimensions

#### Fully / Partially observable
* Fully observable means the sensors give access to complete environment state at each time step
* Effectively fully observable: sensors detect all aspects of the environment that are relevant to a choice of action
  * relevance depends on performance measure
* In fully observable environments we don't need internal state to understand the world 
* Unobservable means the agent has no sensors

#### Single agent / Multiagent
Subtlety: when should agent "A" treat other entities as agents?
* Depends on whether B's behavior is best described as maximizing a performance measure that depends on A's behavior.
* Chess example: 
  * opponent B is trying to maximize performance measure, which means minimizing A's performance measure
  * **competitive** multiagent environment
* Driving example:
  * Avoiding collisions is good for everyone
  * Partially cooperative multiagent environment
  * Partially competitive because only one car can occupy a parking space (for example)
* Agent-design problems in multiagent systems are very different from single agent systems
  * Example: communication emerges as rational behavior
  * Example: randomized behavior can be rational in competitive multiagent environments

#### Deterministic / stochastic
* Deterministic environment: next state is completely determined by the current state.
* Stochastic: not deterministic

Uncertainty:
* Theoretically not a problem in a fully observable deterministic environment.
* Uncertain environment = not fully observable or not deterministic.
* Partially observable environments might _appear_ stochastic.
* For practical purposes, most environments are too complex to keep track of and must be treated as stochastic.

##### Remark
I believe the above comment provides important insight into the meaning of "stochastic". It is not an ontological condition, but rather a modeling convenience arises from limited knowledge, memory and processing power. In other words, it is a sometimes acceptable shorthand.


Stochastic v. Nondeterministic
* Typically stochastic is associated with probabilities.
* Nondeterministic environment descriptions have no probabilities attached.
  * Usually for performance measures that require success for all possible outcomes.

#### Episodic / sequential
* Divide agent experience into atomic time steps.
  * Receive a percept.
  * Perform a single action.
* Episodic: the actions taken in each atom don't depend on previous atoms.
  * Example: classification of assembly line parts
* Sequential: current decision could affect future decisions.
  * Examples: chess and driving

#### Static / dynamic
* Dynamic environments can change while an agent is deliberating.
* Otherwise, environment is static.
* In a dynamic environment, not deciding to act counts as deciding not to act.
* Semidynamic: environment doesn't change, but agents performance score does.

Examples:
* Taxi is dynamic.
* Chess with a clock is semidynamic.
* Chess with no clock is static.

#### Discrete / continuous
Applies to:
* state of environment
* time
* percepts and actions of agent

Examples:
* Chess environment is discrete (except clock)
* Chess percepts and actions are discrete.
* Taxi driving has a continuous state and continuous time.
* Taxi actions are continuous.
* Camera input is discrete "strictly speaking"

##### Remark
Some insight here into the nature of continuity. We mean we have an non-terminating algorithm for specifying the answer with increasing precision. We must therefore make a decision as to when to terminate this algorithm. Camera input is especially interesting. It is discrete because the representation has limited precision. But so does everything in the machine...

#### Known / unknown
* Refers to agent / designer's state of knowledge about the "laws" of the environment.
* Obviously, this isn't a property of the environment itself.
* Known environments: outcomes or outcome probabilities for all actions are given.
* Unknown environments: we don't know (at least initially) what the consequences of actions will be.

#### Notes:
* Hardest case: Partially observable, multiagent, stochastic, sequential, dynamic, continuous, unknown.
* "Not always cut and dried"
* That is, we have to make choices about how we want to model the environment.
* These choices depend on what is relevant to our goals.
* At best, these categories are a rough guide.
* Code repository at aima.cs.berkeley.edu
