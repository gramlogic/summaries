# Good Behavior: The Concept of Rationality

"A rational agent is one that does the right thing"

## The right thing
* consider consequences of behavior on the environment
* a sequence of agent actions and a sequence of environment states
* ask: is the sequence of environment states "desirable"?
* desirability is captured by a **performance measure** that evaluates environment states

### remark
* Performance measure examines "environment states, not agent states"
* N&V think evaluating agent states would mean evaluating "agent's own opinion of its performance" and then go on to some highly tenuous analogy with "sour grapes"
* Of course, the agent's perception of environment states are actually part of the agent's state, so one might think we have to add that the agent is required to trust its sensors and is unable to modify them, or even modify the perception routine to avoid seeing bad things.
* However, it is later revealed (see 2.2.2) that a rational agent is expected to take actions to affect its own percept sequence.
* Consequently, the performance measure must be evaluated externally to the agent and provided as input.
* In other words, we need someone else to check whether the floor is dirty or not.

## performance measure design
* The right thing has been reduced to optimizing a performance measure.
* There is no algorithm for coming up with a performance measure: N&V assign this job to the "designer"
* "not as easy as it sounds" (sounds obviously hard... consider performance measures for humans)
* Examples of vacuum robot

### Rule of thumb
"it is better to design performance measures according to what one actually wants in the environment, rather than according to how one things the agent should behave"
* No explanation for this.

### "knotty issues"
* "might seem to be a fine point of janitorial science, but in fact it is a deep philosophical question with far-reaching implications"
* "Whis is better--a reckless life of highs and lows, or a safe but humdrum existence?"
* "Whis is better--an economy where everyone lives in moderate poverty, or one in which some live in plenty while others are very poor?"
* "We leave these questions as an exercise for the diligent reader."

### A diligent reader responds
These are stupid questions, and their inclusion in this text is baffling. Attributing to them the status of "deep philosophical questions" is revealing of the authors' worldview and poor acquaintance with philosophical texts.
* In order for these questions to be understood as generally important (and not just two examples chosen randomly out of an infinite array of of comparisons), the authors must believe that scarcity is a general principle. That is: you can't have everything you want. We are in competition for limited resources in a zero-sum game, and there is not enough to go around. Only this assumption makes sense of the idea that distributing wealth equally means "everyone lives in moderate poverty".
* Further, where does the word "reckless" come from? A robot that cleans energetically and then rests is not "reckless." It's as though the authors just read the _Mayor of Casterbridge_ and have over generalized it. Or perhaps have been swallowed by a romantic myth of the genius artist. Never heard of Virgil or Thomas Mann... 

## 2.2.1 Rationality

What is rational depends on:
* performance measure
* agent's prior knowledge
* actions that the agent can perform
* agent's percept sequence

### Definition of rational agent:
"For each possible percept sequence, a rational agent should select an action that is expected to maximize its performance measure, given the evidence provided by the percept sequence and whatever built-in knolwedge the agent has."

#### Vacuum example
* performance measure: one point per clean square at each time step over "lifetime" of 1000 steps
* map is known a priori
* not known: initial location of robot, dirt distribution 
* clean squares stay clean.
* left and right do nothing if they would violate boundaries
* actions: left, right, suck
* agent perceives its location
* agent perceives whether its location is dirty

Agent is rational for the above constraints. If we penalize energy use (movement), it would not be rational, etc.

## 2.2.2 Omniscience, learning, and autonomy
"I see an old friend...so, being rational, I decide to cross the street..."
* What is the implied performance measure here?
* I get a point for each time step I spend doing something I enjoy?
* Add "I'm not otherwise engaged" to imply "or doing something I need to do to get something I want?"
* Add "There is no traffic" to imply "without putting myself or others at risk of harm?"

"Our definition of rationality does not require omniscience, then, because the rational choice depends only on the percept sequence _to date_."
* Is this *reason*?
* There is an implied adoption of inductive reasoning: I make a prediction based on past experience.
* Consider David Hume: the laws of logic do not extend to the future.
* Lack of omniscience is not quite the whole problem here.
* Say I observe a robotic swan that always swims 1 m and then turns left. One day it turns right.

### Information gathering
* Rational agents are expected to take actions to modify future percepts
* "Exploring"
* Example is looking before crossing the street.
* The point is that you cannot achieve rationality trivially by ignoring information.
* OK, so should you also look up to check for falling cargo doors?
* If not, why not? How do we decide which checks are rational?
* Are you graded just based on how often you make it across the street?

### Learning
Agent is expected to learn about the environment from what it perceives.
* May have some prior knowledge of the environment.
* This gets augmented and modified.

Examples of agents without learning:
* Dung beetle with dung ball removed.
* Sphex wasp getting stuck in a loop.
  * Dig a burrow.
  * Sting a caterpillar.
  * Drag caterpillar to burrow.
  * Check burrow. (moving the caterpillar at this stage causes the wasp to return to the previous stage)
  * Drag caterpillar inside.
  * Lay eggs.
  * Note this is not well described because the authors refer to the "drag" state but there are two states in the wasps procedure that involve dragging. For their narrative to make sense they must be referring to the first of these two states.

### Autonomy
Independence from prior knowledge given by the program designer.
* Rational agent should be autonomous. (Why? Does this have anything to do with definition of rationality?)
* Claim is that a vacuum that learns to foresee where and when dirt will appear will outperform one that does not.
* This is true, but a vaccum that is told beforehand where and when dirt will appear will do just as well or better.
* The desire for autonomy does not follow from the definition.

#### Rule of thumb and appeal to evolution
* "One seldom requires complete autonomy from the start."
* "Just as evolution provides animals with enough built-in reflexes to survive long enough to learn for themselves, it would be reasonable to provide an artificial intelligent agent with some initial knowledge as well as an ability to learn."
* "After sufficient experiences of its environment, the behavior of a rational agent can become effectively _independent_ of its prior knowledge."
* Thus: learning makes design more general. "allows one to design a single rational agent that will succeed in a vast variety of environments"

In other words, I don't like having to write too many rules or instructions, or answer too many questions.
