# The Structure of Agents
Job:
* Design an agent program that implements the agent function, mapping percepts to actions.
Agent architecture: 
* The platform / physical device with computer, sensors and actuators
Agent = architecture + program
* Hardware and software

## 2.4.1 Agent programs (Overview)
Common Skeleton:
* input = current percept from sensors
* output = action to actuators
* Note that the agent function may use the whole percept sequence, but this has to be remembered.

### remark
* Notes that other skeleton choices exist.
* Example: agent programs as asynchronous coroutines
* This seems likely to be equivalent in terms of computational power (might be more efficient).

Table-driven agent example:
* Pseudo code
* discussion of limitations: obviously the table is too big
* challenge: how to get what we want within space / time constraints
  * Example of success: replacing huge tables of square roots
  * Claim: AI can do for general intelligence what the calculator did for square roots
  * (They say "what Newton did" but this makes no sense)

### Four kinds of agent programs
* Simple reflex agents
* Model-based reflex agents
* Goal-based agents
* Utility-based agents

### Topics for all types of agents
* How all 4 agents can be converted to "learning agents"
* How components within the agent can be represented

## 2.4.2 Simple reflex agents
* No memory
* Select action based on current percept only

vacuum example
* relevant percept sequences are constant instead of exponential
* can be implemented as a Boolean circuit

taxi example:
* fixed condition-action rules = productions = if-then rules
* if percept = P then action = A

### remark
* These "if-then" rules have nothing to do with logical implications.
* There is not much connection to "rationality" here. Or we are seeing a very different definition of rationality.

pros and cons:
* very simple
* only work if the environment is effectively fully observable

randomization:
* Works to break infinite loops for reflex agents in partially observable environments
* Can be a good trick to keep things simple
* Vacuum example: can eliminate location sensor and still work
* "In single-agent environments, randomization is usually _not_ rational."

## 2.4.3 Model-based reflex agents
* Overcome partial observability by keeping track of the world
* Maintain internal state.
* This requires a transition model and a sensor model
* An agent that uses these models to keep track of the state of the world is a **model-based agent.** 

### transition model:
* How the world changes over time
* Both:
  * How agent's actions affect the world
  * How the world evolves on its own

### sensor model:
* Relationship between percepts and state of the world.
* I.e. How to interpret sensor data

### remark
In simulation, we model the world using the simulator, and pass the appropriate percepts to agent sensors. The agent then attempts to create an internal simulation of the world using the resulting percept sequence.

### state (of the world):
* What the world is like now (I think)
* best guess or guesses about a partially observable world
* updated with "update state"
* usually involves uncertainty

## 2.4.4 Goal-based agents
* Keep track of desirable states.
* Combine with model to choose best actions.

### Differences from reflex agents

#### Consider the future:
* Must ask: 
  * what will happen
  * "will it make me happy?"
* Finding the best action sequence to achieve a goal is often non-trivial.
  * Subfields of search and planning 

#### Extensibility
* Explicit goals are easier to modify.
* Example:
  * change the destination (goal-based) vs.
  * change all the turns (reflex)


## 2.4.5 Utility-based agents
* Some ways of achieving goals are better than others.
* Want to distinguish better than "happy" vs "unhappy"
* Better to have levels of happiness

"Because "happy" does not sound very scientific, economists and computer scientists use the term **utility** instead.
* utility refers to quality of being useful

### utility function:
* internalized performance measure
* agent will be rational according a performance measure if 
  * the utility function matches it
  * it tries to maximize utility

#### advantages: 
* utility function is not necessary for rationality, but has advantages 
* flexibility and learning
* better for conflicting goals (when you can't have all of them)
* better for weighing importance of goals against likelihood of sucess

#### expected utility
* the utility the agent expects to derive, on average, given the probabilities and utilities of each outcome

### expected utility and general rationality
* decision making under uncertainty is ubiquitous
* "any rational agent must behave _as if_ it possesses a utility function whose expected value it tries to maximize" (see ch 16)
* "an agent that possesses an _explicit_ utilitity function can make rational decisions with a general-purpose algorithm that does not depend on the specific utility function being maximized" 
* "global" definition of rationality: "agent functions that have the highest performance" get the designation of "rational"
* becomes "local" design constraint

#### remark
* Maximizing an incorrect expected utility function should not be consider irrational. This is a major point for revision.

#### rationality and intelligence (editorial)
* The above gives a distinction between being rational and being smart.
* A rational person is anyone trying to maximize expected utility.
* A smart person either has an expected utility that is closer to actual utility, or has better success at maximizing it, or can maximize it more easily.

#### challenges of utility-based agents
* model / keep track of environment
  * perception, representation, reasoning and learning
* choose the utility-maximizing course of action
* find tractable solutions
* design the utility function itself correctly

#### model-free agents
* Note that these exist.
* Can sometimes avoid need to learn how actions change the environment

## 2.4.6 Learning agents
How do agent programs "come into being"?
* Turing (1950) proposes to build learning machines and teach them.
* Now the preferred method in many areas.

### Advantages
* Less work (maybe) than programming by hand.
* Any type of agent can be built as a learning agent.
* Learning agents can operate in initially unknown environments and improve.

### Four Components

#### performance element: 
takes percepts and selects actions (previously this was the whole agent)

#### learning element:
makes improvements to performance element
  * design depends on design of the performance element, which should be done first.
  * can change any "knowledge" components in agent diagrams
  * simplest case: learn "what actions do" and "how world evolves" from successive pairs in percept sequence

#### critic: 
gives feedback on performance element to learning element
  * uses a fixed performance standard to give feedback
  * important that agent cannot modify performance standard

#### problem generator: 
suggests actions that lead to new informative experiences
  * performance element would prefer to only do what it's good at to get the best score
  * problem generator suggests "exploratory actions":
  * things that are not optimal in themselves but can lead to better longterm outcomes
  * N&V try to draw an analogy to scientists performing experiments (using questionable example of Galileo and tower of Pisa)

#### learning and the performance standard
* Model-based agents: improving the model is usually good, regardless of performance standard
* Need performance standard to improve reflex component or utility function.
* performance standard can designate percepts as reward or penalty; use to update utility function
* Examples:
  * getting tips (or not)
  * human behavior / response to actions as indicators of preferences

#### remark
Animal experience of pain and hunger can be understood as "Hard-wired" performance standards

### Definition of learning
* "a process of modification of each compoent of the agent" 
* "to bring the components into closer agreement with the avilable feedback information"
* "improving overall performance"

## 2.4.7 how the components of agent programs work
Agent program consists of components that answer questions like:
* "What is the world like now?"
* "What action should I do now?"
* "What do my actions do?"

### Basic representation strategies
Ordered by complexity and expressive power:
* **atomic**: state is a black box
* **factored**: state is a vector of attribute values (real, Boolean, strings)
* **structured**: state includes objects, with attributes and relations of their own

#### atomic
Each state of the world is indivisible.
* Example: traveling through cities from one place to another. Current state is the name of the city.
* Applications: standard models for search, game-playing, hidden Markov models, and Markov decision processes

#### factored representation
Each state is split into a fixed set of variables / attributes, each with a value.
* Example: driving problem but this time state includes GPS coordinates, gas, oil light, etc.
* Factored states can have some things in common but not others.
* Applications: constraint satisfaction, propositional logic, planning, Bayesian networks, ML algorithms.

#### structured representation
Objects and their relationships can be described explicitly.
* Example: loose cow is behind a truck so can't back up
* Can't be captured beforehand by creating a special variable for it.
* In other words, state space is vast and sparse.
* Need to be able to generate state description on the fly.
* Applications: Databases, first order logic, first order probability models, natural language understanding.
* "In fact, much of what humans express in natural language concerns objects and their relationships."

#### remark
* It should be possible to express all factored representation as a collection of atomic representations.
* The structured representation must ultimately be made of more primitive relationships.

#### expressiveness
Roughly, a more expressive representation can capture everything a less expressive representation can, plus some.
* Also must be at least as concise.
* Often it's much more concise.
* Example: rules of chess.
  * Can be written in a page or two in first order logic.
  * Require thousands of pages in propositional logic.
  * 10^38 pages in atomic language like finite-state automata (i.e. where the position of every piece on the board is a single atomic state--note that this involves representing lots of positions that can never happen)
* Reasoning and learning get more complicated as expressive power increases
* Intelligent systems may need to use multiple representations at the same time.

### Representation and memory

#### localist representation
One-to-one mapping between concepts and memory locations.
* Mapping from concept to memory is arbitrary.
* If a few bits are wrong you get a totally different concept.

#### distributed representation
* Representation of a concept is spread over many memory locations.
* Each memory location is employed as part of multiple concepts.
* More robust against noise / data loss.
* A concept is a point in multidimensional space.
* A few bits off moves to a similar concept.
